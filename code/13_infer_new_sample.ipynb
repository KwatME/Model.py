{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:49.777639Z",
     "start_time": "2018-07-06T07:59:49.765242Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment import *\n",
    "\n",
    "SETTING = kraft.read_json(\"setting.json\")\n",
    "\n",
    "PATH = make_path_dict(SETTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x_sample_1 = pd.read_csv(\n",
    "    SETTING[\"new_feature_x_sample_file_path\"], sep=\"\\t\", index_col=0\n",
    ")\n",
    "\n",
    "feature_x_sample_1.index.name = SETTING[\"feature_alias\"]\n",
    "\n",
    "feature_x_sample_1.columns.name = SETTING[\"new_sample_alias\"]\n",
    "\n",
    "summarize_feature_x_sample_keyword_arguments = {\n",
    "    \"feature_x_sample_alias\": SETTING[\"feature_x_sample_alias\"],\n",
    "    \"feature_x_sample_value_name\": SETTING[\"feature_x_sample_value_name\"],\n",
    "    \"plot_heat_map_max_size\": SETTING[\"plot_heat_map_max_size\"],\n",
    "    \"plot_histogram_max_size\": SETTING[\"plot_histogram_max_size\"],\n",
    "    \"plot_rug_max_size\": SETTING[\"plot_rug_max_size\"],\n",
    "    \"plot\": True,\n",
    "}\n",
    "\n",
    "feature_x_sample_processed_1 = kraft.process_feature_x_sample(\n",
    "    feature_x_sample_1,\n",
    "    features_to_drop=SETTING[\"new_features_to_drop\"],\n",
    "    samples_to_drop=SETTING[\"new_samples_to_drop\"],\n",
    "    nanize=SETTING[\"new_nanize\"],\n",
    "    drop_axis=SETTING[\"new_drop_axis\"],\n",
    "    max_na=SETTING[\"new_max_na\"],\n",
    "    min_n_not_na_value=SETTING[\"new_min_n_not_na_value\"],\n",
    "    min_n_not_na_unique_value=SETTING[\"new_min_n_not_na_unique_value\"],\n",
    "    shift_as_necessary_to_achieve_min_before_logging=SETTING[\n",
    "        \"new_shift_as_necessary_to_achieve_min_before_logging\"\n",
    "    ],\n",
    "    log_base=SETTING[\"new_log_base\"],\n",
    "    normalization_axis=SETTING[\"new_normalization_axis\"],\n",
    "    normalization_method=SETTING[\"new_normalization_method\"],\n",
    "    clip_min=SETTING[\"new_clip_min\"],\n",
    "    clip_max=SETTING[\"new_clip_max\"],\n",
    "    **summarize_feature_x_sample_keyword_arguments,\n",
    ")\n",
    "\n",
    "feature_x_sample_processed_0 = pd.read_csv(\n",
    "    PATH[\"feature_x_sample.processed.tsv\"], sep=\"\\t\", index_col=0\n",
    ")\n",
    "\n",
    "feature_x_sample_processed_0.columns.name = SETTING[\"sample_alias\"]\n",
    "\n",
    "w_0 = pd.read_csv(PATH[\"w.tsv\"], sep=\"\\t\", index_col=0)\n",
    "\n",
    "w_0.columns.name = \"Factor\"\n",
    "\n",
    "h_0 = pd.read_csv(PATH[\"h.tsv\"], sep=\"\\t\", index_col=0)\n",
    "\n",
    "h_0.columns.name = SETTING[\"sample_alias\"]\n",
    "\n",
    "gps_map_0 = kraft.read_gps_map(PATH[\"gps_map.pickle.gz\"])\n",
    "\n",
    "feature_x_sample_processed_1 = feature_x_sample_processed_1.reindex(\n",
    "    index=pd.Index(w_0.index.str.lstrip(\"(-+) \").unique(), name=w_0.index.name)\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"{feature_x_sample_processed_1.isna().all(axis=1).sum() / feature_x_sample_processed_1.shape[0]:.2%} feature are missing.\"\n",
    ")\n",
    "\n",
    "output_directory_path = os.path.join(\n",
    "    PATH[\"infer/\"], SETTING[\"new_feature_x_sample_alias\"]\n",
    ")\n",
    "\n",
    "kraft.establish_path(output_directory_path, \"directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTING[\"signal_type\"] == \"raw\":\n",
    "\n",
    "    def make_raw_signal(series, signal_normalization_method, using):\n",
    "\n",
    "        if using is None:\n",
    "\n",
    "            return series\n",
    "\n",
    "        elif using == \"training\":\n",
    "\n",
    "            _1d_array = feature_x_sample_processed_0.loc[series.name].values\n",
    "\n",
    "        elif using == \"testing\":\n",
    "\n",
    "            _1d_array = series.values\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError(f\"using can be None, training, or testing.\")\n",
    "\n",
    "        _1d_array_good = _1d_array[\n",
    "            ~kraft.check_nd_array_for_bad(_1d_array, raise_for_bad=False)\n",
    "        ]\n",
    "\n",
    "        if _1d_array_good.size == 0:\n",
    "\n",
    "            return pd.Series(index=series.index, name=series.name)\n",
    "\n",
    "        elif signal_normalization_method == \"0-1\":\n",
    "\n",
    "            min_ = _1d_array_good.min()\n",
    "\n",
    "            max_ = _1d_array_good.max()\n",
    "\n",
    "            return (series - min_) / (max_ - min_)\n",
    "\n",
    "    feature_x_sample_signal_1 = feature_x_sample_processed_1.apply(\n",
    "        make_raw_signal,\n",
    "        axis=SETTING[\"signal_normalization_axis\"],\n",
    "        signal_normalization_method=SETTING[\"signal_normalization_method\"],\n",
    "        using=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTING[\"signal_type\"] == \"context\":\n",
    "\n",
    "    inconsistent_features = kraft.select_series_indices(\n",
    "        feature_x_sample_processed_1.apply(\n",
    "            lambda feature_values: abs(\n",
    "                feature_values.median()\n",
    "                - feature_x_sample_processed_0.loc[feature_values.name].median()\n",
    "            ),\n",
    "            axis=1,\n",
    "        ),\n",
    "        \">\",\n",
    "        fraction=SETTING[\"new_inconsistent_feature_fraction_to_drop\"],\n",
    "        title={\"text\": SETTING[\"feature_alias\"]},\n",
    "        xaxis={\"title\": \"Rank\"},\n",
    "        yaxis={\"title\": \"Median Difference\"},\n",
    "    ).tolist()\n",
    "\n",
    "    extend = []\n",
    "\n",
    "    for inconsistent_feature in inconsistent_features:\n",
    "\n",
    "        for template in (\"(-) {}\", \"(+) {}\"):\n",
    "\n",
    "            extend.append(template.format(inconsistent_feature))\n",
    "\n",
    "    inconsistent_features += extend\n",
    "\n",
    "    w_0.drop(w_0.index & inconsistent_features, inplace=True)\n",
    "\n",
    "    feature_x_sample_signal_1 = pd.DataFrame(\n",
    "        index=w_0.index, columns=feature_x_sample_processed_1.columns\n",
    "    )\n",
    "\n",
    "    feature_x_fit_parameter_0 = pd.read_csv(\n",
    "        PATH[\"feature_x_fit_parameter.tsv\"], sep=\"\\t\", index_col=0\n",
    "    )\n",
    "\n",
    "    n = w_0.shape[0]\n",
    "\n",
    "    n_per_print = n // 10\n",
    "\n",
    "    for i, sign_feature in enumerate(w_0.index):\n",
    "\n",
    "        if i % n_per_print == 0:\n",
    "\n",
    "            print(f\"{i + 1}/{n} ...\")\n",
    "\n",
    "        sign, feature = sign_feature.split()\n",
    "\n",
    "        n_data_0, location_0, scale_0, degree_of_freedom_0, shape_0 = feature_x_fit_parameter_0.loc[\n",
    "            feature\n",
    "        ]\n",
    "\n",
    "        context_dict_0 = kraft.compute_1d_array_context(\n",
    "            feature_x_sample_processed_0.loc[feature].values,\n",
    "            n_data=n_data_0,\n",
    "            location=location_0,\n",
    "            scale=scale_0,\n",
    "            degree_of_freedom=degree_of_freedom_0,\n",
    "            shape=shape_0,\n",
    "        )\n",
    "\n",
    "        grid_0 = context_dict_0[\"grid\"]\n",
    "\n",
    "        context_indices_0 = context_dict_0[\"context\"]\n",
    "\n",
    "        if sign == \"(-)\":\n",
    "\n",
    "            signals_0 = -context_indices_0.clip(max=0)\n",
    "\n",
    "        elif sign == \"(+)\":\n",
    "\n",
    "            signals_0 = context_indices_0.clip(min=0)\n",
    "\n",
    "            signals_0 = kraft.normalize_nd_array(\n",
    "                signals_0,\n",
    "                None,\n",
    "                SETTING[\"signal_normalization_method\"],\n",
    "                raise_for_bad=False,\n",
    "            )\n",
    "\n",
    "        values_1 = feature_x_sample_processed_1.loc[feature].values\n",
    "\n",
    "        is_good_1 = ~kraft.check_nd_array_for_bad(values_1, raise_for_bad=False)\n",
    "\n",
    "        feature_x_sample_signal_1.iloc[i, is_good_1] = signals_0[\n",
    "            [np.absolute(value_1 - grid_0).argmin() for value_1 in values_1[is_good_1]]\n",
    "        ]\n",
    "\n",
    "    for sign_feature in np.random.choice(\n",
    "        feature_x_sample_signal_1.index, size=8, replace=False\n",
    "    ):\n",
    "\n",
    "        feature = sign_feature.split()[1]\n",
    "\n",
    "        n_data_0, location_0, scale_0, degree_of_freedom_0, shape_0 = feature_x_fit_parameter_0.loc[\n",
    "            feature\n",
    "        ]\n",
    "\n",
    "        kraft.plot_context(\n",
    "            feature_x_sample_processed_0.loc[feature],\n",
    "            n_data=n_data_0,\n",
    "            location=location_0,\n",
    "            scale=scale_0,\n",
    "            degree_of_freedom=degree_of_freedom_0,\n",
    "            shape=shape_0,\n",
    "            title=sign_feature,\n",
    "        )\n",
    "\n",
    "        values_1 = feature_x_sample_processed_1.loc[feature].sort_values()\n",
    "\n",
    "        kraft.plot_and_save(\n",
    "            {\n",
    "                \"layout\": {\"title\": {\"text\": f\"{sign_feature} Context in New Data\"}},\n",
    "                \"data\": [\n",
    "                    {\n",
    "                        \"type\": \"scatter\",\n",
    "                        \"x\": values_1,\n",
    "                        \"y\": feature_x_sample_signal_1.loc[\n",
    "                            sign_feature, values_1.index\n",
    "                        ],\n",
    "                        \"text\": values_1.index,\n",
    "                        \"mode\": \"markers\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = f\"{SETTING['feature_x_sample_alias']}<br>Infers<br>{SETTING['new_feature_x_sample_alias']} (n={feature_x_sample_signal_1.shape[1]})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:52.556033Z",
     "start_time": "2018-07-06T07:59:52.348450Z"
    }
   },
   "outputs": [],
   "source": [
    "h_1 = kraft.solve_for_H(feature_x_sample_signal_1.fillna(0), w_0)\n",
    "\n",
    "h_1_file_path = os.path.join(output_directory_path, \"h.tsv\")\n",
    "\n",
    "h_1.to_csv(h_1_file_path, sep=\"\\t\")\n",
    "\n",
    "if h_1.shape[1] < 16:\n",
    "\n",
    "    function = kraft.plot_bubble_map\n",
    "\n",
    "else:\n",
    "\n",
    "    function = kraft.plot_heat_map\n",
    "\n",
    "dataframe = kraft.normalize_series_or_dataframe(h_1, 0, \"-0-\")\n",
    "\n",
    "if dataframe.shape[0] < SETTING[\"plot_cluster_max_size\"]:\n",
    "\n",
    "    dataframe = dataframe.iloc[kraft.cluster_2d_array(dataframe.values, 0)]\n",
    "\n",
    "if dataframe.shape[1] < SETTING[\"plot_cluster_max_size\"]:\n",
    "\n",
    "    dataframe = dataframe.iloc[:, kraft.cluster_2d_array(dataframe.values, 1)]\n",
    "\n",
    "function(\n",
    "    dataframe,\n",
    "    title=title,\n",
    "    xaxis_title=h_1.columns.name,\n",
    "    yaxis_title=h_1.index.name,\n",
    "    html_file_path=h_1_file_path.replace(\".tsv\", \".html\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_arguments = (\"h\", h_1)\n",
    "\n",
    "predict_keyword_arguments = {\n",
    "    \"n_pull\": SETTING[\"gps_map_h_n_pull\"],\n",
    "    \"pull_power\": SETTING[\"gps_map_h_pull_power\"],\n",
    "    \"element_marker_size\": SETTING[\"gps_map_h_element_marker_size\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:52.556033Z",
     "start_time": "2018-07-06T07:59:52.348450Z"
    }
   },
   "outputs": [],
   "source": [
    "gps_map_0.predict(\n",
    "    *predict_arguments,\n",
    "    title=title,\n",
    "    html_file_path=os.path.join(output_directory_path, \"gps_map.html\"),\n",
    "    **predict_keyword_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_x_sample = pd.read_csv(\n",
    "    \"~/garden/data/densely_interconnected_transcriptional_circuits_control_cell_states_in_human_hematopoiesis/binary_information_x_sample.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "for annotation_name, sample_value in annotation_x_sample.iterrows():\n",
    "\n",
    "    gps_map_0.predict(\n",
    "        *predict_arguments,\n",
    "        annotation_x_element=sample_value.to_frame().T,\n",
    "        title=f\"{annotation_name} (n={sample_value.sum()})\",\n",
    "        html_file_path=os.path.join(\n",
    "            output_directory_path,\n",
    "            kraft.normalize_file_name(f\"gps_map.{annotation_name}.html\"),\n",
    "        ),\n",
    "        **predict_keyword_arguments,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
