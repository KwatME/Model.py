{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:49.777639Z",
     "start_time": "2018-07-06T07:59:49.765242Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __init__ import *\n",
    "\n",
    "PROJECT_JSON = kraft.read_json(\"../project.json\")\n",
    "\n",
    "PATH = make_path_dict(PROJECT_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x_sample_1 = pd.read_csv(\n",
    "    PROJECT_JSON[\"new_feature_x_sample_file_path\"], sep=\"\\t\", index_col=0\n",
    ")\n",
    "\n",
    "feature_x_sample_1.index.name = PROJECT_JSON[\"feature_alias\"]\n",
    "\n",
    "feature_x_sample_1.columns.name = PROJECT_JSON[\"new_sample_alias\"]\n",
    "\n",
    "summarize_feature_x_sample_keyword_arguments = {\n",
    "    \"feature_x_sample_alias\": PROJECT_JSON[\"feature_x_sample_alias\"],\n",
    "    \"feature_x_sample_value_name\": PROJECT_JSON[\"feature_x_sample_value_name\"],\n",
    "    \"plot_heat_map_max_size\": PROJECT_JSON[\"plot_heat_map_max_size\"],\n",
    "    \"plot_histogram_max_size\": PROJECT_JSON[\"plot_histogram_max_size\"],\n",
    "    \"plot_rug_max_size\": PROJECT_JSON[\"plot_rug_max_size\"],\n",
    "    \"plot\": False,\n",
    "}\n",
    "\n",
    "feature_x_sample_processed_1 = kraft.process_feature_x_sample(\n",
    "    feature_x_sample_1,\n",
    "    features_to_drop=PROJECT_JSON[\"new_features_to_drop\"],\n",
    "    samples_to_drop=PROJECT_JSON[\"new_samples_to_drop\"],\n",
    "    nanize=PROJECT_JSON[\"new_nanize\"],\n",
    "    drop_axis=PROJECT_JSON[\"new_drop_axis\"],\n",
    "    max_na=PROJECT_JSON[\"new_max_na\"],\n",
    "    min_n_not_na_value=PROJECT_JSON[\"new_min_n_not_na_value\"],\n",
    "    min_n_not_na_unique_value=PROJECT_JSON[\"new_min_n_not_na_unique_value\"],\n",
    "    shift_as_necessary_to_achieve_min_before_logging=PROJECT_JSON[\n",
    "        \"new_shift_as_necessary_to_achieve_min_before_logging\"\n",
    "    ],\n",
    "    log_base=PROJECT_JSON[\"new_log_base\"],\n",
    "    normalization_axis=PROJECT_JSON[\"new_normalization_axis\"],\n",
    "    normalization_method=PROJECT_JSON[\"new_normalization_method\"],\n",
    "    clip_min=PROJECT_JSON[\"new_clip_min\"],\n",
    "    clip_max=PROJECT_JSON[\"new_clip_max\"],\n",
    "    **summarize_feature_x_sample_keyword_arguments,\n",
    ")\n",
    "\n",
    "feature_x_sample_processed_0 = pd.read_csv(\n",
    "    PATH[\"feature_x_sample.processed.tsv\"], sep=\"\\t\", index_col=0\n",
    ")\n",
    "\n",
    "feature_x_sample_processed_0.columns.name = PROJECT_JSON[\"sample_alias\"]\n",
    "\n",
    "w_0 = pd.read_csv(PATH[\"w.tsv\"], sep=\"\\t\", index_col=0)\n",
    "\n",
    "w_0.columns.name = \"Factor\"\n",
    "\n",
    "h_0 = pd.read_csv(PATH[\"h.tsv\"], sep=\"\\t\", index_col=0)\n",
    "\n",
    "h_0.columns.name = PROJECT_JSON[\"sample_alias\"]\n",
    "\n",
    "gps_map_0 = kraft.read_gps_map(PATH[\"gps_map.pickle.gz\"])\n",
    "\n",
    "feature_x_sample_processed_1 = feature_x_sample_processed_1.reindex(\n",
    "    index=pd.Index(w_0.index.str.lstrip(\"(-+) \").unique(), name=w_0.index.name)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"{:.2%} feature are missing.\".format(\n",
    "        feature_x_sample_processed_1.isna().all(axis=1).sum()\n",
    "        / feature_x_sample_processed_1.shape[0]\n",
    "    )\n",
    ")\n",
    "\n",
    "output_directory_path = os.path.join(\n",
    "    PATH[\"infer/\"], PROJECT_JSON[\"new_feature_x_sample_alias\"]\n",
    ")\n",
    "\n",
    "kraft.establish_path(output_directory_path, \"directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_JSON[\"signal_type\"] == \"raw\":\n",
    "\n",
    "    def make_raw_signal(series, signal_normalization_method, using):\n",
    "\n",
    "        if using is None:\n",
    "\n",
    "            return series\n",
    "\n",
    "        elif using == \"training\":\n",
    "\n",
    "            vector = feature_x_sample_processed_0.loc[series.name].values\n",
    "\n",
    "        elif using == \"testing\":\n",
    "\n",
    "            vector = series.values\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise\n",
    "\n",
    "        vector_good = vector[~kraft.check_array_for_bad(vector, raise_for_bad=False)]\n",
    "\n",
    "        if vector_good.size == 0:\n",
    "\n",
    "            return pd.Series(index=series.index, name=series.name)\n",
    "\n",
    "        elif signal_normalization_method == \"0-1\":\n",
    "\n",
    "            min_ = vector_good.min()\n",
    "\n",
    "            max_ = vector_good.max()\n",
    "\n",
    "            return (series - min_) / (max_ - min_)\n",
    "\n",
    "    feature_x_sample_signal_1 = feature_x_sample_processed_1.apply(\n",
    "        make_raw_signal,\n",
    "        axis=PROJECT_JSON[\"signal_normalization_axis\"],\n",
    "        signal_normalization_method=PROJECT_JSON[\"signal_normalization_method\"],\n",
    "        using=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_JSON[\"signal_type\"] == \"context\":\n",
    "\n",
    "    inconsistent_features = kraft.select_series_indices(\n",
    "        feature_x_sample_processed_1.apply(\n",
    "            lambda feature_values: abs(\n",
    "                feature_values.median()\n",
    "                - feature_x_sample_processed_0.loc[feature_values.name].median()\n",
    "            ),\n",
    "            axis=1,\n",
    "        ),\n",
    "        \">\",\n",
    "        fraction=PROJECT_JSON[\"new_inconsistent_feature_fraction_to_drop\"],\n",
    "        title={\"text\": PROJECT_JSON[\"feature_alias\"]},\n",
    "        yaxis={\"title\": \"Median Difference\"},\n",
    "    ).tolist()\n",
    "\n",
    "    extend = []\n",
    "\n",
    "    for inconsistent_feature in inconsistent_features:\n",
    "\n",
    "        for template in (\"(-) {}\", \"(+) {}\"):\n",
    "\n",
    "            extend.append(template.format(inconsistent_feature))\n",
    "\n",
    "    inconsistent_features += extend\n",
    "\n",
    "    w_0.drop(w_0.index & inconsistent_features, inplace=True)\n",
    "\n",
    "    feature_x_sample_signal_1 = pd.DataFrame(\n",
    "        index=w_0.index, columns=feature_x_sample_processed_1.columns\n",
    "    )\n",
    "\n",
    "    feature_x_fit_parameter_0 = pd.read_csv(\n",
    "        PATH[\"feature_x_fit_parameter.tsv\"], sep=\"\\t\", index_col=0\n",
    "    )\n",
    "\n",
    "    n = w_0.shape[0]\n",
    "\n",
    "    n_per_print = n // 10\n",
    "\n",
    "    for i, sign_feature in enumerate(w_0.index):\n",
    "\n",
    "        if i % n_per_print == 0:\n",
    "\n",
    "            print(\"{}/{}...\".format(i + 1, n))\n",
    "\n",
    "        sign, feature = sign_feature.split()\n",
    "\n",
    "        n_data_0, location_0, scale_0, degree_of_freedom_0, shape_0 = feature_x_fit_parameter_0.loc[\n",
    "            feature\n",
    "        ]\n",
    "\n",
    "        context_dict_0 = kraft.compute_vector_context(\n",
    "            feature_x_sample_processed_0.loc[feature].values,\n",
    "            n_data=n_data_0,\n",
    "            location=location_0,\n",
    "            scale=scale_0,\n",
    "            degree_of_freedom=degree_of_freedom_0,\n",
    "            shape=shape_0,\n",
    "        )\n",
    "\n",
    "        grid_0 = context_dict_0[\"grid\"]\n",
    "\n",
    "        context_indices_0 = context_dict_0[\"context\"]\n",
    "\n",
    "        if sign == \"(-)\":\n",
    "\n",
    "            signals_0 = -context_indices_0.clip(max=0)\n",
    "\n",
    "        elif sign == \"(+)\":\n",
    "\n",
    "            signals_0 = context_indices_0.clip(min=0)\n",
    "\n",
    "            signals_0 = kraft.normalize_array(\n",
    "                signals_0,\n",
    "                None,\n",
    "                PROJECT_JSON[\"signal_normalization_method\"],\n",
    "                raise_for_bad=False,\n",
    "            )\n",
    "\n",
    "        values_1 = feature_x_sample_processed_1.loc[feature].values\n",
    "\n",
    "        is_good_1 = ~kraft.check_array_for_bad(values_1, raise_for_bad=False)\n",
    "\n",
    "        feature_x_sample_signal_1.iloc[i, is_good_1] = signals_0[\n",
    "            [np.absolute(value_1 - grid_0).argmin() for value_1 in values_1[is_good_1]]\n",
    "        ]\n",
    "\n",
    "    for sign_feature in np.random.choice(\n",
    "        feature_x_sample_signal_1.index, size=8, replace=False\n",
    "    ):\n",
    "\n",
    "        feature = sign_feature.split()[1]\n",
    "\n",
    "        n_data_0, location_0, scale_0, degree_of_freedom_0, shape_0 = feature_x_fit_parameter_0.loc[\n",
    "            feature\n",
    "        ]\n",
    "\n",
    "        kraft.plot_context(\n",
    "            feature_x_sample_processed_0.loc[feature],\n",
    "            n_data=n_data_0,\n",
    "            location=location_0,\n",
    "            scale=scale_0,\n",
    "            degree_of_freedom=degree_of_freedom_0,\n",
    "            shape=shape_0,\n",
    "            title=sign_feature,\n",
    "        )\n",
    "\n",
    "        values_1 = feature_x_sample_processed_1.loc[feature].sort_values()\n",
    "\n",
    "        kraft.plot_plotly_figure(\n",
    "            {\n",
    "                \"layout\": {\n",
    "                    \"title\": {\"text\": \"{} Context in New Data\".format(sign_feature)}\n",
    "                },\n",
    "                \"data\": [\n",
    "                    {\n",
    "                        \"type\": \"scatter\",\n",
    "                        \"x\": values_1,\n",
    "                        \"y\": feature_x_sample_signal_1.loc[\n",
    "                            sign_feature, values_1.index\n",
    "                        ],\n",
    "                        \"text\": values_1.index,\n",
    "                        \"mode\": \"markers\",\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "            None,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"{}<br>Infers<br>{} (n={})\".format(\n",
    "    PROJECT_JSON[\"feature_x_sample_alias\"],\n",
    "    PROJECT_JSON[\"new_feature_x_sample_alias\"],\n",
    "    feature_x_sample_signal_1.shape[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:52.556033Z",
     "start_time": "2018-07-06T07:59:52.348450Z"
    }
   },
   "outputs": [],
   "source": [
    "h_1 = kraft.solve_for_h(feature_x_sample_signal_1.fillna(0), w_0)\n",
    "\n",
    "h_1_file_path = os.path.join(output_directory_path, \"h.tsv\")\n",
    "\n",
    "h_1.to_csv(h_1_file_path, sep=\"\\t\")\n",
    "\n",
    "if h_1.shape[1] < 16:\n",
    "\n",
    "    function = kraft.plot_bubble_map\n",
    "\n",
    "else:\n",
    "\n",
    "    function = kraft.plot_heat_map\n",
    "\n",
    "dataframe = kraft.normalize_dataframe(h_1, 0, \"-0-\")\n",
    "\n",
    "if dataframe.shape[0] < PROJECT_JSON[\"plot_cluster_max_size\"]:\n",
    "\n",
    "    dataframe = dataframe.iloc[kraft.cluster_matrix(dataframe.values, 0)]\n",
    "\n",
    "if dataframe.shape[1] < PROJECT_JSON[\"plot_cluster_max_size\"]:\n",
    "\n",
    "    dataframe = dataframe.iloc[:, kraft.cluster_matrix(dataframe.values, 1)]\n",
    "\n",
    "function(\n",
    "    dataframe,\n",
    "    title_text=title,\n",
    "    xaxis_title_text=h_1.columns.name,\n",
    "    yaxis_title_text=h_1.index.name,\n",
    "    html_file_path=h_1_file_path.replace(\".tsv\", \".html\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_arguments = (\"h\", h_1)\n",
    "\n",
    "predict_keyword_arguments = {\n",
    "    \"n_pull\": PROJECT_JSON[\"gps_map_h_n_pull\"],\n",
    "    \"pull_power\": PROJECT_JSON[\"gps_map_h_pull_power\"],\n",
    "    \"element_marker_size\": PROJECT_JSON[\"new_gps_map_h_element_marker_size\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:52.556033Z",
     "start_time": "2018-07-06T07:59:52.348450Z"
    }
   },
   "outputs": [],
   "source": [
    "gps_map_0.predict(\n",
    "    *predict_arguments,\n",
    "    title=title,\n",
    "    html_file_path=os.path.join(output_directory_path, \"gps_map.html\"),\n",
    "    **predict_keyword_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_data_dicts = PROJECT_JSON[\"new_sample_data\"]\n",
    "\n",
    "new_sample_data_dicts = {\n",
    "    data_name: data_dict\n",
    "    for data_name, data_dict in new_sample_data_dicts.items()\n",
    "    if \"indices_to_peek\" in data_dict\n",
    "}\n",
    "\n",
    "for data_name, data_dict in new_sample_data_dicts.items():\n",
    "\n",
    "    print(data_name)\n",
    "\n",
    "    data_dict[\"dataframe\"] = pd.read_csv(data_dict[\"file_path\"], sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name, data_dict in new_sample_data_dicts.items():\n",
    "\n",
    "    indices_to_peek = data_dict[\"indices_to_peek\"]\n",
    "\n",
    "    if indices_to_peek == \"all\":\n",
    "\n",
    "        indices_to_peek = data_dict[\"dataframe\"].index\n",
    "\n",
    "    data_type = data_dict[\"type\"]\n",
    "\n",
    "    if PROJECT_JSON[\"plot_std\"] is None:\n",
    "\n",
    "        annotation_std_maxs = None\n",
    "\n",
    "    else:\n",
    "\n",
    "        annotation_std_maxs = (PROJECT_JSON[\"plot_std\"],)\n",
    "\n",
    "    if data_type == \"categorical\":\n",
    "\n",
    "        annotation_colorscales = (\"Portland\",)\n",
    "\n",
    "    else:\n",
    "\n",
    "        annotation_colorscales = None\n",
    "\n",
    "    for index, element_value in (\n",
    "        data_dict[\"dataframe\"]\n",
    "        .loc[data_dict[\"dataframe\"].index & indices_to_peek]\n",
    "        .iterrows()\n",
    "    ):\n",
    "\n",
    "        gps_map_file_name = kraft.normalize_file_name(\n",
    "            \"gps_map.{}.{}.html\".format(data_name, index)\n",
    "        )\n",
    "\n",
    "        print(gps_map_file_name)\n",
    "\n",
    "        gps_map_0.predict(\n",
    "            *predict_arguments,\n",
    "            annotation_x_element=element_value.to_frame().T,\n",
    "            annotation_types=(data_type,),\n",
    "            annotation_std_maxs=annotation_std_maxs,\n",
    "            annotation_colorscales=annotation_colorscales,\n",
    "            title=\"({}) {}\".format(data_name, index),\n",
    "            html_file_path=os.path.join(\n",
    "                output_directory_path,\n",
    "                kraft.normalize_file_name(\n",
    "                    \"gps_map.{}.{}.html\".format(data_name, index)\n",
    "                ),\n",
    "            ),\n",
    "            **predict_keyword_arguments,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
