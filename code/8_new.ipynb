{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:49.777639Z",
     "start_time": "2018-07-06T07:59:49.765242Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T20:13:16.628911Z",
     "start_time": "2018-07-06T20:13:14.935143Z"
    }
   },
   "outputs": [],
   "source": [
    "from __init__ import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x_sample_1 = pd.read_csv(\n",
    "    SETTING[\"new_feature_x_sample_file_path\"], sep=\"\\t\", index_col=0\n",
    ")\n",
    "\n",
    "feature_x_sample_1.columns.name = SETTING[\"new_sample_alias\"]\n",
    "\n",
    "summarize_feature_x_sample_keyword_arguments = {\n",
    "    \"feature_x_sample_alias\": SETTING[\"new_feature_x_sample_alias\"],\n",
    "    \"feature_x_sample_value_name\": SETTING[\"feature_x_sample_value_name\"],\n",
    "}\n",
    "\n",
    "feature_x_sample_processed_1 = kraft.process_feature_x_sample(\n",
    "    feature_x_sample_1,\n",
    "    features_to_drop=SETTING[\"new_features_to_drop\"],\n",
    "    samples_to_drop=SETTING[\"new_samples_to_drop\"],\n",
    "    nanize=SETTING[\"new_nanize\"],\n",
    "    drop_axis=SETTING[\"new_drop_axis\"],\n",
    "    max_na=SETTING[\"new_max_na\"],\n",
    "    min_n_not_na_value=SETTING[\"new_min_n_not_na_value\"],\n",
    "    min_n_not_na_unique_value=SETTING[\"new_min_n_not_na_unique_value\"],\n",
    "    shift_as_necessary_to_achieve_min_before_logging=SETTING[\n",
    "        \"new_shift_as_necessary_to_achieve_min_before_logging\"\n",
    "    ],\n",
    "    log_base=SETTING[\"new_log_base\"],\n",
    "    normalization_axis=SETTING[\"new_normalization_axis\"],\n",
    "    normalization_method=SETTING[\"new_normalization_method\"],\n",
    "    clip_min=SETTING[\"new_clip_min\"],\n",
    "    clip_max=SETTING[\"new_clip_max\"],\n",
    "    **summarize_feature_x_sample_keyword_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x_sample_processed_0 = pd.read_csv(\n",
    "    PATH[\"feature_x_sample.processed.tsv\"], sep=\"\\t\", index_col=0\n",
    ")\n",
    "\n",
    "feature_x_sample_processed_0.columns.name = SETTING[\"sample_alias\"]\n",
    "\n",
    "w_0 = pd.read_csv(PATH[\"w.tsv\"], sep=\"\\t\", index_col=0)\n",
    "\n",
    "h_0 = pd.read_csv(PATH[\"h.tsv\"], sep=\"\\t\", index_col=0)\n",
    "\n",
    "w_0.columns.name = h_0.index.name\n",
    "\n",
    "h_0.columns.name = SETTING[\"sample_alias\"]\n",
    "\n",
    "gps_map_0 = kraft.read_gps_map(PATH[\"gps_map.pickle.gz\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_x_sample_processed_1 = feature_x_sample_processed_1.reindex(\n",
    "    index=pd.Index(w_0.index.str.lstrip(\"(-+) \").unique(), name=w_0.index.name)\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Fraction missing feature: {:.2e}\".format(\n",
    "        feature_x_sample_processed_1.isna().all(axis=1).sum()\n",
    "        / feature_x_sample_processed_1.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory_path = os.path.join(\n",
    "    PATH[\"infer/\"], SETTING[\"new_feature_x_sample_alias\"]\n",
    ")\n",
    "\n",
    "kraft.establish_path(output_directory_path, \"directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTING[\"signal_type\"] == \"raw\":\n",
    "\n",
    "    def make_raw_signal(series, signal_normalization_method, using):\n",
    "\n",
    "        if using is None:\n",
    "\n",
    "            return series\n",
    "\n",
    "        elif using == \"training\":\n",
    "\n",
    "            vector = feature_x_sample_processed_0.loc[series.name].values\n",
    "\n",
    "        elif using == \"testing\":\n",
    "\n",
    "            vector = series.values\n",
    "\n",
    "        vector_good = vector[~kraft.check_array_for_bad(vector, raise_if_bad=False)]\n",
    "\n",
    "        if vector_good.size == 0:\n",
    "\n",
    "            return pd.Series(index=series.index, name=series.name)\n",
    "\n",
    "        elif signal_normalization_method == \"0-1\":\n",
    "\n",
    "            vector_good_min = vector_good.min()\n",
    "\n",
    "            return (series - vector_good_min) / (vector_good.max() - vector_good_min)\n",
    "\n",
    "    feature_x_sample_signal_1 = feature_x_sample_processed_1.apply(\n",
    "        make_raw_signal,\n",
    "        axis=SETTING[\"signal_normalization_axis\"],\n",
    "        signal_normalization_method=SETTING[\"signal_normalization_method\"],\n",
    "        using=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SETTING[\"signal_type\"] == \"context\":\n",
    "\n",
    "    inconsistent_features = kraft.select_series_indices(\n",
    "        feature_x_sample_processed_1.apply(\n",
    "            lambda feature_values: abs(\n",
    "                feature_values.median()\n",
    "                - feature_x_sample_processed_0.loc[feature_values.name].median()\n",
    "            ),\n",
    "            axis=1,\n",
    "        ),\n",
    "        \">\",\n",
    "        fraction=SETTING[\"new_inconsistent_feature_fraction_to_drop\"],\n",
    "        layout={\"yaxis\": {\"title\": {\"text\": \"Median Difference\"}}},\n",
    "    ).tolist()\n",
    "\n",
    "    extend = []\n",
    "\n",
    "    for inconsistent_feature in inconsistent_features:\n",
    "\n",
    "        for template in (\"(-) {}\", \"(+) {}\"):\n",
    "\n",
    "            extend.append(template.format(inconsistent_feature))\n",
    "\n",
    "    inconsistent_features += extend\n",
    "\n",
    "    w_0.drop(inconsistent_features, errors=\"ignore\", inplace=True)\n",
    "\n",
    "    feature_x_sample_signal_1 = pd.DataFrame(\n",
    "        index=w_0.index, columns=feature_x_sample_processed_1.columns\n",
    "    )\n",
    "\n",
    "    feature_x_fit_parameter_0 = pd.read_csv(\n",
    "        PATH[\"feature_x_fit_parameter.tsv\"], sep=\"\\t\", index_col=0\n",
    "    )\n",
    "\n",
    "    n = w_0.shape[0]\n",
    "\n",
    "    n_per_print = n // 10\n",
    "\n",
    "    for i, sign_feature in enumerate(w_0.index):\n",
    "\n",
    "        if i % n_per_print == 0:\n",
    "\n",
    "            print(\"{}/{}...\".format(i + 1, n))\n",
    "\n",
    "        sign, feature = sign_feature.split()\n",
    "\n",
    "        (\n",
    "            n_data_0,\n",
    "            location_0,\n",
    "            scale_0,\n",
    "            degree_of_freedom_0,\n",
    "            shape_0,\n",
    "        ) = feature_x_fit_parameter_0.loc[feature]\n",
    "\n",
    "        context_dict_0 = kraft.compute_vector_context(\n",
    "            feature_x_sample_processed_0.loc[feature].values,\n",
    "            n_data=n_data_0,\n",
    "            location=location_0,\n",
    "            scale=scale_0,\n",
    "            degree_of_freedom=degree_of_freedom_0,\n",
    "            shape=shape_0,\n",
    "        )\n",
    "\n",
    "        grid_0 = context_dict_0[\"grid\"]\n",
    "\n",
    "        context_indices_0 = context_dict_0[\"context\"]\n",
    "\n",
    "        if sign == \"(-)\":\n",
    "\n",
    "            signals_0 = -context_indices_0.clip(max=0)\n",
    "\n",
    "        elif sign == \"(+)\":\n",
    "\n",
    "            signals_0 = context_indices_0.clip(min=0)\n",
    "\n",
    "            signals_0 = kraft.normalize_array(\n",
    "                signals_0,\n",
    "                SETTING[\"signal_normalization_axis\"],\n",
    "                SETTING[\"signal_normalization_method\"],\n",
    "                raise_if_bad=False,\n",
    "            )\n",
    "\n",
    "        values_1 = feature_x_sample_processed_1.loc[feature].values\n",
    "\n",
    "        is_good_1 = ~kraft.check_array_for_bad(values_1, raise_if_bad=False)\n",
    "\n",
    "        feature_x_sample_signal_1.iloc[i, is_good_1] = signals_0[\n",
    "            [np.absolute(value_1 - grid_0).argmin() for value_1 in values_1[is_good_1]]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout = {\n",
    "    \"title\": {\n",
    "        \"text\": \"{} Infers {} (n={})\".format(\n",
    "            SETTING[\"feature_x_sample_alias\"],\n",
    "            SETTING[\"new_feature_x_sample_alias\"],\n",
    "            feature_x_sample_signal_1.shape[1],\n",
    "        )\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:52.556033Z",
     "start_time": "2018-07-06T07:59:52.348450Z"
    }
   },
   "outputs": [],
   "source": [
    "h_1 = kraft.solve_v_wh_h(feature_x_sample_signal_1.fillna(value=0), w_0)\n",
    "\n",
    "h_1_file_path = os.path.join(output_directory_path, \"h.tsv\")\n",
    "\n",
    "h_1.to_csv(h_1_file_path, sep=\"\\t\")\n",
    "\n",
    "if h_1.shape[1] < 30:\n",
    "\n",
    "    function = kraft.plot_bubble_map\n",
    "\n",
    "else:\n",
    "\n",
    "    function = kraft.plot_heat_map\n",
    "\n",
    "dataframe = kraft.normalize_dataframe(h_1, 0, \"-0-\")\n",
    "\n",
    "dataframe = dataframe.iloc[\n",
    "    kraft.cluster_matrix(dataframe.values, 0), kraft.cluster_matrix(dataframe.values, 1)\n",
    "]\n",
    "\n",
    "function(\n",
    "    dataframe, layout=layout, html_file_path=h_1_file_path.replace(\".tsv\", \".html\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_keyword_arguments = {\n",
    "    \"n_pull\": SETTING[\"gps_map_h_n_pull\"],\n",
    "    \"pull_power\": SETTING[\"gps_map_h_pull_power\"],\n",
    "    \"element_marker_size\": SETTING[\"new_gps_map_h_element_marker_size\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T07:59:52.556033Z",
     "start_time": "2018-07-06T07:59:52.348450Z"
    }
   },
   "outputs": [],
   "source": [
    "gps_map_0.predict(\n",
    "    \"h\",\n",
    "    h_1,\n",
    "    layout=layout,\n",
    "    html_file_path=os.path.join(output_directory_path, \"gps_map.html\"),\n",
    "    **predict_keyword_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sample_data_dicts = SETTING[\"new_sample_data\"]\n",
    "\n",
    "new_sample_data_dicts = {\n",
    "    data_name: data_dict\n",
    "    for data_name, data_dict in new_sample_data_dicts.items()\n",
    "    if \"indices_to_peek\" in data_dict\n",
    "}\n",
    "\n",
    "for data_name, data_dict in new_sample_data_dicts.items():\n",
    "\n",
    "    print(data_name)\n",
    "\n",
    "    data_dict[\"dataframe\"] = pd.read_csv(data_dict[\"file_path\"], sep=\"\\t\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_name, data_dict in new_sample_data_dicts.items():\n",
    "\n",
    "    indices_to_peek = data_dict[\"indices_to_peek\"]\n",
    "\n",
    "    if indices_to_peek == \"all\":\n",
    "\n",
    "        indices_to_peek = data_dict[\"dataframe\"].index\n",
    "\n",
    "    for index, element_value in (\n",
    "        data_dict[\"dataframe\"]\n",
    "        .reindex(index=indices_to_peek)\n",
    "        .dropna(how=\"all\")\n",
    "        .iterrows()\n",
    "    ):\n",
    "\n",
    "        gps_map_0.predict(\n",
    "            \"h\",\n",
    "            h_1,\n",
    "            element_value=element_value,\n",
    "            element_value_data_type=data_dict[\"data_type\"],\n",
    "            html_file_path=os.path.join(\n",
    "                output_directory_path,\n",
    "                kraft.normalize_file_name(\n",
    "                    \"gps_map.{}.{}.html\".format(data_name, index)\n",
    "                ),\n",
    "            ),\n",
    "            **predict_keyword_arguments,\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
